{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1760012,"sourceType":"datasetVersion","datasetId":1046158}],"dockerImageVersionId":30042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CROP RECOMMENDATION SYSTEM ","metadata":{}},{"cell_type":"code","source":"\nfrom __future__ import print_function\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn import tree\n\nimport warnings\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:51:49.722155Z","iopub.execute_input":"2024-02-01T06:51:49.722570Z","iopub.status.idle":"2024-02-01T06:51:49.730345Z","shell.execute_reply.started":"2024-02-01T06:51:49.722535Z","shell.execute_reply":"2024-02-01T06:51:49.729072Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"def load_dataset(data_path):\n    df=pd.read_csv(data_path)\n    return df\ndata_path='/kaggle/input/crop-recommendation-dataset/Crop_recommendation.csv'\ndf=load_dataset(data_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:51:50.747050Z","iopub.execute_input":"2024-02-01T06:51:50.747633Z","iopub.status.idle":"2024-02-01T06:51:50.762730Z","shell.execute_reply.started":"2024-02-01T06:51:50.747594Z","shell.execute_reply":"2024-02-01T06:51:50.761355Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:51:51.521547Z","iopub.execute_input":"2024-02-01T06:51:51.521968Z","iopub.status.idle":"2024-02-01T06:51:51.542525Z","shell.execute_reply.started":"2024-02-01T06:51:51.521933Z","shell.execute_reply":"2024-02-01T06:51:51.541554Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"        N   P   K  temperature   humidity        ph    rainfall   label\n0      90  42  43    20.879744  82.002744  6.502985  202.935536    rice\n1      85  58  41    21.770462  80.319644  7.038096  226.655537    rice\n2      60  55  44    23.004459  82.320763  7.840207  263.964248    rice\n3      74  35  40    26.491096  80.158363  6.980401  242.864034    rice\n4      78  42  42    20.130175  81.604873  7.628473  262.717340    rice\n...   ...  ..  ..          ...        ...       ...         ...     ...\n2195  107  34  32    26.774637  66.413269  6.780064  177.774507  coffee\n2196   99  15  27    27.417112  56.636362  6.086922  127.924610  coffee\n2197  118  33  30    24.131797  67.225123  6.362608  173.322839  coffee\n2198  117  32  34    26.272418  52.127394  6.758793  127.175293  coffee\n2199  104  18  30    23.603016  60.396475  6.779833  140.937041  coffee\n\n[2200 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90</td>\n      <td>42</td>\n      <td>43</td>\n      <td>20.879744</td>\n      <td>82.002744</td>\n      <td>6.502985</td>\n      <td>202.935536</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85</td>\n      <td>58</td>\n      <td>41</td>\n      <td>21.770462</td>\n      <td>80.319644</td>\n      <td>7.038096</td>\n      <td>226.655537</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>55</td>\n      <td>44</td>\n      <td>23.004459</td>\n      <td>82.320763</td>\n      <td>7.840207</td>\n      <td>263.964248</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>74</td>\n      <td>35</td>\n      <td>40</td>\n      <td>26.491096</td>\n      <td>80.158363</td>\n      <td>6.980401</td>\n      <td>242.864034</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>78</td>\n      <td>42</td>\n      <td>42</td>\n      <td>20.130175</td>\n      <td>81.604873</td>\n      <td>7.628473</td>\n      <td>262.717340</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2195</th>\n      <td>107</td>\n      <td>34</td>\n      <td>32</td>\n      <td>26.774637</td>\n      <td>66.413269</td>\n      <td>6.780064</td>\n      <td>177.774507</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2196</th>\n      <td>99</td>\n      <td>15</td>\n      <td>27</td>\n      <td>27.417112</td>\n      <td>56.636362</td>\n      <td>6.086922</td>\n      <td>127.924610</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2197</th>\n      <td>118</td>\n      <td>33</td>\n      <td>30</td>\n      <td>24.131797</td>\n      <td>67.225123</td>\n      <td>6.362608</td>\n      <td>173.322839</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>117</td>\n      <td>32</td>\n      <td>34</td>\n      <td>26.272418</td>\n      <td>52.127394</td>\n      <td>6.758793</td>\n      <td>127.175293</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2199</th>\n      <td>104</td>\n      <td>18</td>\n      <td>30</td>\n      <td>23.603016</td>\n      <td>60.396475</td>\n      <td>6.779833</td>\n      <td>140.937041</td>\n      <td>coffee</td>\n    </tr>\n  </tbody>\n</table>\n<p>2200 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dropped_column = df.pop('label')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:51:52.617606Z","iopub.execute_input":"2024-02-01T06:51:52.618030Z","iopub.status.idle":"2024-02-01T06:51:52.623740Z","shell.execute_reply.started":"2024-02-01T06:51:52.617987Z","shell.execute_reply":"2024-02-01T06:51:52.622660Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"#df=df.drop(\"label\",axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(df)\ndf= pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\nprint(\"All features are now scaled\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:52:17.230739Z","iopub.execute_input":"2024-02-01T06:52:17.231195Z","iopub.status.idle":"2024-02-01T06:52:17.247433Z","shell.execute_reply.started":"2024-02-01T06:52:17.231157Z","shell.execute_reply":"2024-02-01T06:52:17.246445Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"All features are now scaled\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:48:02.923643Z","iopub.execute_input":"2024-02-01T06:48:02.924075Z","iopub.status.idle":"2024-02-01T06:48:02.941765Z","shell.execute_reply.started":"2024-02-01T06:48:02.924038Z","shell.execute_reply":"2024-02-01T06:48:02.940731Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"          N         P         K  temperature  humidity        ph  rainfall\n0  1.068797 -0.344551 -0.101688    -0.935587  0.472666  0.043302  1.810361\n1  0.933329  0.140616 -0.141185    -0.759646  0.397051  0.734873  2.242058\n2  0.255986  0.049647 -0.081939    -0.515898  0.486954  1.771510  2.921066\n3  0.635298 -0.556811 -0.160933     0.172807  0.389805  0.660308  2.537048\n4  0.743673 -0.344551 -0.121436    -1.083647  0.454792  1.497868  2.898373","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.068797</td>\n      <td>-0.344551</td>\n      <td>-0.101688</td>\n      <td>-0.935587</td>\n      <td>0.472666</td>\n      <td>0.043302</td>\n      <td>1.810361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.933329</td>\n      <td>0.140616</td>\n      <td>-0.141185</td>\n      <td>-0.759646</td>\n      <td>0.397051</td>\n      <td>0.734873</td>\n      <td>2.242058</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.255986</td>\n      <td>0.049647</td>\n      <td>-0.081939</td>\n      <td>-0.515898</td>\n      <td>0.486954</td>\n      <td>1.771510</td>\n      <td>2.921066</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.635298</td>\n      <td>-0.556811</td>\n      <td>-0.160933</td>\n      <td>0.172807</td>\n      <td>0.389805</td>\n      <td>0.660308</td>\n      <td>2.537048</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.743673</td>\n      <td>-0.344551</td>\n      <td>-0.121436</td>\n      <td>-1.083647</td>\n      <td>0.454792</td>\n      <td>1.497868</td>\n      <td>2.898373</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:48:03.898453Z","iopub.execute_input":"2024-02-01T06:48:03.898855Z","iopub.status.idle":"2024-02-01T06:48:03.915349Z","shell.execute_reply.started":"2024-02-01T06:48:03.898821Z","shell.execute_reply":"2024-02-01T06:48:03.913819Z"},"trusted":true},"execution_count":112,"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"             N         P         K  temperature  humidity        ph  rainfall\n2195  1.529390 -0.587134 -0.318922     0.228814 -0.227709  0.401395  1.352437\n2196  1.312641 -1.163269 -0.417666     0.355720 -0.666947 -0.494413  0.445183\n2197  1.827421 -0.617457 -0.358420    -0.293218 -0.191235 -0.138120  1.271418\n2198  1.800327 -0.647780 -0.279425     0.129612 -0.869518  0.373904  0.431545\n2199  1.448109 -1.072300 -0.358420    -0.397667 -0.498020  0.401096  0.682005","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2195</th>\n      <td>1.529390</td>\n      <td>-0.587134</td>\n      <td>-0.318922</td>\n      <td>0.228814</td>\n      <td>-0.227709</td>\n      <td>0.401395</td>\n      <td>1.352437</td>\n    </tr>\n    <tr>\n      <th>2196</th>\n      <td>1.312641</td>\n      <td>-1.163269</td>\n      <td>-0.417666</td>\n      <td>0.355720</td>\n      <td>-0.666947</td>\n      <td>-0.494413</td>\n      <td>0.445183</td>\n    </tr>\n    <tr>\n      <th>2197</th>\n      <td>1.827421</td>\n      <td>-0.617457</td>\n      <td>-0.358420</td>\n      <td>-0.293218</td>\n      <td>-0.191235</td>\n      <td>-0.138120</td>\n      <td>1.271418</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>1.800327</td>\n      <td>-0.647780</td>\n      <td>-0.279425</td>\n      <td>0.129612</td>\n      <td>-0.869518</td>\n      <td>0.373904</td>\n      <td>0.431545</td>\n    </tr>\n    <tr>\n      <th>2199</th>\n      <td>1.448109</td>\n      <td>-1.072300</td>\n      <td>-0.358420</td>\n      <td>-0.397667</td>\n      <td>-0.498020</td>\n      <td>0.401096</td>\n      <td>0.682005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.size","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:48:04.698656Z","iopub.execute_input":"2024-02-01T06:48:04.699079Z","iopub.status.idle":"2024-02-01T06:48:04.707241Z","shell.execute_reply.started":"2024-02-01T06:48:04.699043Z","shell.execute_reply":"2024-02-01T06:48:04.706304Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"15400"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:48:05.286473Z","iopub.execute_input":"2024-02-01T06:48:05.287021Z","iopub.status.idle":"2024-02-01T06:48:05.295048Z","shell.execute_reply.started":"2024-02-01T06:48:05.286953Z","shell.execute_reply":"2024-02-01T06:48:05.293405Z"},"trusted":true},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"(2200, 7)"},"metadata":{}}]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:48:05.859023Z","iopub.execute_input":"2024-02-01T06:48:05.859477Z","iopub.status.idle":"2024-02-01T06:48:05.867186Z","shell.execute_reply.started":"2024-02-01T06:48:05.859435Z","shell.execute_reply":"2024-02-01T06:48:05.866070Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"Index(['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.concat([df, dropped_column], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:52:38.838081Z","iopub.execute_input":"2024-02-01T06:52:38.838626Z","iopub.status.idle":"2024-02-01T06:52:38.845080Z","shell.execute_reply.started":"2024-02-01T06:52:38.838589Z","shell.execute_reply":"2024-02-01T06:52:38.844111Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"df['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:52:41.461897Z","iopub.execute_input":"2024-02-01T06:52:41.462316Z","iopub.status.idle":"2024-02-01T06:52:41.470994Z","shell.execute_reply.started":"2024-02-01T06:52:41.462280Z","shell.execute_reply":"2024-02-01T06:52:41.469587Z"},"trusted":true},"execution_count":127,"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"array(['rice', 'maize', 'chickpea', 'kidneybeans', 'pigeonpeas',\n       'mothbeans', 'mungbean', 'blackgram', 'lentil', 'pomegranate',\n       'banana', 'mango', 'grapes', 'watermelon', 'muskmelon', 'apple',\n       'orange', 'papaya', 'coconut', 'cotton', 'jute', 'coffee'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:52:59.202744Z","iopub.execute_input":"2024-02-01T06:52:59.203200Z","iopub.status.idle":"2024-02-01T06:52:59.227728Z","shell.execute_reply.started":"2024-02-01T06:52:59.203161Z","shell.execute_reply":"2024-02-01T06:52:59.226773Z"},"trusted":true},"execution_count":130,"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"             N         P         K  temperature  humidity        ph  rainfall  \\\n0     1.068797 -0.344551 -0.101688    -0.935587  0.472666  0.043302  1.810361   \n1     0.933329  0.140616 -0.141185    -0.759646  0.397051  0.734873  2.242058   \n2     0.255986  0.049647 -0.081939    -0.515898  0.486954  1.771510  2.921066   \n3     0.635298 -0.556811 -0.160933     0.172807  0.389805  0.660308  2.537048   \n4     0.743673 -0.344551 -0.121436    -1.083647  0.454792  1.497868  2.898373   \n...        ...       ...       ...          ...       ...       ...       ...   \n2195  1.529390 -0.587134 -0.318922     0.228814 -0.227709  0.401395  1.352437   \n2196  1.312641 -1.163269 -0.417666     0.355720 -0.666947 -0.494413  0.445183   \n2197  1.827421 -0.617457 -0.358420    -0.293218 -0.191235 -0.138120  1.271418   \n2198  1.800327 -0.647780 -0.279425     0.129612 -0.869518  0.373904  0.431545   \n2199  1.448109 -1.072300 -0.358420    -0.397667 -0.498020  0.401096  0.682005   \n\n       label  \n0       rice  \n1       rice  \n2       rice  \n3       rice  \n4       rice  \n...      ...  \n2195  coffee  \n2196  coffee  \n2197  coffee  \n2198  coffee  \n2199  coffee  \n\n[2200 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.068797</td>\n      <td>-0.344551</td>\n      <td>-0.101688</td>\n      <td>-0.935587</td>\n      <td>0.472666</td>\n      <td>0.043302</td>\n      <td>1.810361</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.933329</td>\n      <td>0.140616</td>\n      <td>-0.141185</td>\n      <td>-0.759646</td>\n      <td>0.397051</td>\n      <td>0.734873</td>\n      <td>2.242058</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.255986</td>\n      <td>0.049647</td>\n      <td>-0.081939</td>\n      <td>-0.515898</td>\n      <td>0.486954</td>\n      <td>1.771510</td>\n      <td>2.921066</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.635298</td>\n      <td>-0.556811</td>\n      <td>-0.160933</td>\n      <td>0.172807</td>\n      <td>0.389805</td>\n      <td>0.660308</td>\n      <td>2.537048</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.743673</td>\n      <td>-0.344551</td>\n      <td>-0.121436</td>\n      <td>-1.083647</td>\n      <td>0.454792</td>\n      <td>1.497868</td>\n      <td>2.898373</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2195</th>\n      <td>1.529390</td>\n      <td>-0.587134</td>\n      <td>-0.318922</td>\n      <td>0.228814</td>\n      <td>-0.227709</td>\n      <td>0.401395</td>\n      <td>1.352437</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2196</th>\n      <td>1.312641</td>\n      <td>-1.163269</td>\n      <td>-0.417666</td>\n      <td>0.355720</td>\n      <td>-0.666947</td>\n      <td>-0.494413</td>\n      <td>0.445183</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2197</th>\n      <td>1.827421</td>\n      <td>-0.617457</td>\n      <td>-0.358420</td>\n      <td>-0.293218</td>\n      <td>-0.191235</td>\n      <td>-0.138120</td>\n      <td>1.271418</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>1.800327</td>\n      <td>-0.647780</td>\n      <td>-0.279425</td>\n      <td>0.129612</td>\n      <td>-0.869518</td>\n      <td>0.373904</td>\n      <td>0.431545</td>\n      <td>coffee</td>\n    </tr>\n    <tr>\n      <th>2199</th>\n      <td>1.448109</td>\n      <td>-1.072300</td>\n      <td>-0.358420</td>\n      <td>-0.397667</td>\n      <td>-0.498020</td>\n      <td>0.401096</td>\n      <td>0.682005</td>\n      <td>coffee</td>\n    </tr>\n  </tbody>\n</table>\n<p>2200 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:06.432046Z","iopub.execute_input":"2024-02-01T06:53:06.432599Z","iopub.status.idle":"2024-02-01T06:53:06.441090Z","shell.execute_reply.started":"2024-02-01T06:53:06.432561Z","shell.execute_reply":"2024-02-01T06:53:06.440114Z"},"trusted":true},"execution_count":131,"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"N              float64\nP              float64\nK              float64\ntemperature    float64\nhumidity       float64\nph             float64\nrainfall       float64\nlabel           object\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:06.737228Z","iopub.execute_input":"2024-02-01T06:53:06.738146Z","iopub.status.idle":"2024-02-01T06:53:06.753405Z","shell.execute_reply.started":"2024-02-01T06:53:06.738089Z","shell.execute_reply":"2024-02-01T06:53:06.751898Z"},"trusted":true},"execution_count":132,"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"grapes         100\ncoffee         100\ncoconut        100\nmaize          100\nchickpea       100\npigeonpeas     100\napple          100\nmungbean       100\ncotton         100\nkidneybeans    100\nrice           100\npomegranate    100\nmothbeans      100\nblackgram      100\nlentil         100\nbanana         100\npapaya         100\norange         100\nmango          100\nwatermelon     100\njute           100\nmuskmelon      100\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### Seperating features and target label","metadata":{}},{"cell_type":"code","source":"features = df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]\ntarget = df['label']\nlabels = df['label']","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:09.576900Z","iopub.execute_input":"2024-02-01T06:53:09.577316Z","iopub.status.idle":"2024-02-01T06:53:09.586020Z","shell.execute_reply.started":"2024-02-01T06:53:09.577275Z","shell.execute_reply":"2024-02-01T06:53:09.584396Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"# Initializing empty lists to append all model's name and corresponding name\nacc = []\nmodel = []","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:11.082934Z","iopub.execute_input":"2024-02-01T06:53:11.083326Z","iopub.status.idle":"2024-02-01T06:53:11.088041Z","shell.execute_reply.started":"2024-02-01T06:53:11.083293Z","shell.execute_reply":"2024-02-01T06:53:11.086951Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"# Splitting into train and test data\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:12.136623Z","iopub.execute_input":"2024-02-01T06:53:12.137050Z","iopub.status.idle":"2024-02-01T06:53:12.146295Z","shell.execute_reply.started":"2024-02-01T06:53:12.137011Z","shell.execute_reply":"2024-02-01T06:53:12.145035Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree\nStep-1: Begin the tree with the root node, says S, which contains the complete dataset.\n\nStep-2: Find the best attribute in the dataset using Attribute Selection Measure (ASM).\n\nStep-3: Divide the S into subsets that contains possible values for the best attributes.\n\nStep-4: Generate the decision tree node, which contains the best attribute.\n\nStep-5: Recursively make new decision trees using the subsets of the dataset created in step -3. Continue this process until a stage is reached where you cannot further classify the nodes and called the final node as a leaf node.\n\n\nEvaluation measures: \n\n1)gini index:\n-Gini index is a measure of impurity or purity used while creating a decision tree in the CART(Classification and Regression Tree) algorithm.\n\n-An attribute with the low Gini index should be preferred as compared to the high Gini index.\n\n-It only creates binary splits, and the CART algorithm uses the Gini index to create binary splits.\n\n2)information gain:\n-It calculates how much information a feature provides us about a class.\n\n-A decision tree algorithm always tries to maximize the value of information gain, and a node/attribute having the highest information gain is split first.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nDecisionTree = DecisionTreeClassifier(criterion=\"entropy\",random_state=2,max_depth=5)\n\n#criterion are of 2 types : entropy and gini impurity\n\n'''\nrandom_state = With random_state=None , we get different train and test sets across \ndifferent executions and the shuffling process is out of control. With random_state=0 , \nwe get the same train and test sets across different executions.\n'''\n\n#max_depth = depth of the decision tree\n\nDecisionTree.fit(Xtrain,Ytrain)\n\npredicted_values = DecisionTree.predict(Xtest)\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('Decision Tree')\nprint(\"DecisionTrees's Accuracy is: \", x*100)\n\nprint(classification_report(Ytest,predicted_values))\n\n'''\nCONFUSION MATRIX\n\nThe confusion matrix is a table that summarizes how successful the classification model \nis at predicting examples belonging to various classes. One axis of the confusion matrix is \nthe label that the model predicted, and the other axis is the actual label.\n\nAccuracy \nnumber of correct predictions over all predictions\n\nPrecision \nPrecision is the ratio of correctly predicted positive observations to the total predicted \npositives, emphasizing the accuracy of positive predictions.\n\nRecall is the ratio of correctly predicted positive observations to the total actual positives,\nhighlighting the ability to capture all positive instances.\n\nF1 score is the harmonic mean of precision and recall, providing a balanced measure of a \nmodel's performance on positive predictions.the harmonic mean is a way of combining precision \nand recall into a single metric.\n'''","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:39.977479Z","iopub.execute_input":"2024-02-01T05:40:39.977911Z","iopub.status.idle":"2024-02-01T05:40:40.049974Z","shell.execute_reply.started":"2024-02-01T05:40:39.977875Z","shell.execute_reply":"2024-02-01T05:40:40.048655Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"DecisionTrees's Accuracy is:  90.0\n              precision    recall  f1-score   support\n\n       apple       1.00      1.00      1.00        13\n      banana       1.00      1.00      1.00        17\n   blackgram       0.59      1.00      0.74        16\n    chickpea       1.00      1.00      1.00        21\n     coconut       0.91      1.00      0.95        21\n      coffee       1.00      1.00      1.00        22\n      cotton       1.00      1.00      1.00        20\n      grapes       1.00      1.00      1.00        18\n        jute       0.74      0.93      0.83        28\n kidneybeans       0.00      0.00      0.00        14\n      lentil       0.68      1.00      0.81        23\n       maize       1.00      1.00      1.00        21\n       mango       1.00      1.00      1.00        26\n   mothbeans       0.00      0.00      0.00        19\n    mungbean       1.00      1.00      1.00        24\n   muskmelon       1.00      1.00      1.00        23\n      orange       1.00      1.00      1.00        29\n      papaya       1.00      0.84      0.91        19\n  pigeonpeas       0.62      1.00      0.77        18\n pomegranate       1.00      1.00      1.00        17\n        rice       1.00      0.62      0.77        16\n  watermelon       1.00      1.00      1.00        15\n\n    accuracy                           0.90       440\n   macro avg       0.84      0.88      0.85       440\nweighted avg       0.86      0.90      0.87       440\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"\"\\nCONFUSION MATRIX\\n\\nThe confusion matrix is a table that summarizes how successful the classification model \\nis at predicting examples belonging to various classes. One axis of the confusion matrix is \\nthe label that the model predicted, and the other axis is the actual label.\\n\\nAccuracy \\nnumber of correct predictions over all predictions\\n\\nPrecision \\nPrecision is the ratio of correctly predicted positive observations to the total predicted \\npositives, emphasizing the accuracy of positive predictions.\\n\\nRecall is the ratio of correctly predicted positive observations to the total actual positives,\\nhighlighting the ability to capture all positive instances.\\n\\nF1 score is the harmonic mean of precision and recall, providing a balanced measure of a \\nmodel's performance on positive predictions.the harmonic mean is a way of combining precision \\nand recall into a single metric.\\n\""},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:42.200171Z","iopub.execute_input":"2024-02-01T05:40:42.200684Z","iopub.status.idle":"2024-02-01T05:40:42.206687Z","shell.execute_reply.started":"2024-02-01T05:40:42.200633Z","shell.execute_reply":"2024-02-01T05:40:42.205448Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Cross validation score (Decision Tree)\nscore = cross_val_score(DecisionTree, features, target,cv=5)\n\n'''\nDecisionTree: This is the decision tree classifier that you have previously defined. \nIt will be used to train the model during each iteration of the cross-validation.\n\nfeatures: These are the input features used for training the model.\n\ntarget: This is the target variable or labels that the model is trying to predict.\n\ncv=5: This parameter specifies the number of folds in the cross-validation process. \nIn this case, cv=5 means that the dataset will be split into 5 folds, and the cross-validation \nprocess will be repeated 5 times. During each iteration, the model is trained on 4 folds and \nvalidated on the remaining fold.\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:42.883197Z","iopub.execute_input":"2024-02-01T05:40:42.883867Z","iopub.status.idle":"2024-02-01T05:40:43.065033Z","shell.execute_reply.started":"2024-02-01T05:40:42.883784Z","shell.execute_reply":"2024-02-01T05:40:43.063743Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'\\nDecisionTree: This is the decision tree classifier that you have previously defined. \\nIt will be used to train the model during each iteration of the cross-validation.\\n\\nfeatures: These are the input features used for training the model.\\n\\ntarget: This is the target variable or labels that the model is trying to predict.\\n\\ncv=5: This parameter specifies the number of folds in the cross-validation process. \\nIn this case, cv=5 means that the dataset will be split into 5 folds, and the cross-validation \\nprocess will be repeated 5 times. During each iteration, the model is trained on 4 folds and \\nvalidated on the remaining fold.\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"score","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:43.702600Z","iopub.execute_input":"2024-02-01T05:40:43.703125Z","iopub.status.idle":"2024-02-01T05:40:43.711763Z","shell.execute_reply.started":"2024-02-01T05:40:43.703078Z","shell.execute_reply":"2024-02-01T05:40:43.710387Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"array([0.93636364, 0.90909091, 0.91818182, 0.87045455, 0.93636364])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving trained Decision Tree model","metadata":{}},{"cell_type":"code","source":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nDT_pkl_filename = 'DecisionTree.pkl'\n# Open the file to save as pkl file\nDT_Model_pkl = open(DT_pkl_filename, 'wb')\npickle.dump(DecisionTree, DT_Model_pkl)\n# Close the pickle instances\nDT_Model_pkl.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:45.210430Z","iopub.execute_input":"2024-02-01T05:40:45.211063Z","iopub.status.idle":"2024-02-01T05:40:45.220471Z","shell.execute_reply.started":"2024-02-01T05:40:45.211021Z","shell.execute_reply":"2024-02-01T05:40:45.218686Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Guassian Naive Bayes\n\nIt is mainly used in text classification that includes a high-dimensional training dataset.\n\nSteps:\n\nConvert the given dataset into frequency tables.\n\nGenerate Likelihood table by finding the probabilities of given features.\n\nNow, use Bayes theorem to calculate the posterior probability.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nNaiveBayes = GaussianNB()\n\nNaiveBayes.fit(Xtrain,Ytrain)\n\npredicted_values = NaiveBayes.predict(Xtest)\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('Naive Bayes')\nprint(\"Naive Bayes's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:46.603366Z","iopub.execute_input":"2024-02-01T05:40:46.603952Z","iopub.status.idle":"2024-02-01T05:40:46.653248Z","shell.execute_reply.started":"2024-02-01T05:40:46.603900Z","shell.execute_reply":"2024-02-01T05:40:46.651945Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Naive Bayes's Accuracy is:  0.990909090909091\n              precision    recall  f1-score   support\n\n       apple       1.00      1.00      1.00        13\n      banana       1.00      1.00      1.00        17\n   blackgram       1.00      1.00      1.00        16\n    chickpea       1.00      1.00      1.00        21\n     coconut       1.00      1.00      1.00        21\n      coffee       1.00      1.00      1.00        22\n      cotton       1.00      1.00      1.00        20\n      grapes       1.00      1.00      1.00        18\n        jute       0.88      1.00      0.93        28\n kidneybeans       1.00      1.00      1.00        14\n      lentil       1.00      1.00      1.00        23\n       maize       1.00      1.00      1.00        21\n       mango       1.00      1.00      1.00        26\n   mothbeans       1.00      1.00      1.00        19\n    mungbean       1.00      1.00      1.00        24\n   muskmelon       1.00      1.00      1.00        23\n      orange       1.00      1.00      1.00        29\n      papaya       1.00      1.00      1.00        19\n  pigeonpeas       1.00      1.00      1.00        18\n pomegranate       1.00      1.00      1.00        17\n        rice       1.00      0.75      0.86        16\n  watermelon       1.00      1.00      1.00        15\n\n    accuracy                           0.99       440\n   macro avg       0.99      0.99      0.99       440\nweighted avg       0.99      0.99      0.99       440\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cross validation score (NaiveBayes)\nscore = cross_val_score(NaiveBayes,features,target,cv=5)\nscore","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:47.614844Z","iopub.execute_input":"2024-02-01T05:40:47.615605Z","iopub.status.idle":"2024-02-01T05:40:47.732106Z","shell.execute_reply.started":"2024-02-01T05:40:47.615560Z","shell.execute_reply":"2024-02-01T05:40:47.731053Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"array([0.99772727, 0.99545455, 0.99545455, 0.99545455, 0.99090909])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving trained Guassian Naive Bayes model","metadata":{}},{"cell_type":"code","source":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nNB_pkl_filename = 'NBClassifier.pkl'\n# Open the file to save as pkl file\nNB_Model_pkl = open(NB_pkl_filename, 'wb')\npickle.dump(NaiveBayes, NB_Model_pkl)\n# Close the pickle instances\nNB_Model_pkl.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:40:49.235956Z","iopub.execute_input":"2024-02-01T05:40:49.236696Z","iopub.status.idle":"2024-02-01T05:40:49.242987Z","shell.execute_reply.started":"2024-02-01T05:40:49.236650Z","shell.execute_reply":"2024-02-01T05:40:49.241965Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine (SVM)\n\nSVM algorithm helps to find the best line or decision boundary; this best boundary or region is called as a hyperplane. SVM algorithm finds the closest point of the lines from both the classes. These points are called support vectors. The distance between the vectors and the hyperplane is called as margin. And the goal of SVM is to maximize this margin. The hyperplane with maximum margin is called the optimal hyperplane.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nSVM = SVC(gamma='auto')\n\n'''\nGamma parameter: The gamma parameter in SVMs is a hyperparameter that controls the shape \nof the decision boundary. It determines the flexibility of the model and the level of \noverfitting or underfitting of the training data.\n\nTypes:\n---> scale:The default setting for the gamma parameter in scikit-learn SVM.\nIt is calculated as 1 / (n_features * X.var()), where X is the input data.\nThis type of scaling is useful when features have similar magnitudes.\n\n---> auto:Similar to 'scale', but it uses the entire dataset's variance instead of individual \nfeatures. It can be a good choice when the dataset is relatively small.\n\n---> float:If a specific numerical value is provided (e.g., 0.1, 1.0, 10.0), it directly \nsets the value of gamma.\nA smaller value will result in a smoother decision boundary, while a larger value will make \nthe boundary more sensitive to individual data points.\n'''\n\nSVM.fit(Xtrain,Ytrain)\n\npredicted_values = SVM.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('SVM')\nprint(\"SVM's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:55:35.133558Z","iopub.execute_input":"2024-02-01T05:55:35.134011Z","iopub.status.idle":"2024-02-01T05:55:35.568485Z","shell.execute_reply.started":"2024-02-01T05:55:35.133972Z","shell.execute_reply":"2024-02-01T05:55:35.567434Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"SVM's Accuracy is:  0.10681818181818181\n              precision    recall  f1-score   support\n\n       apple       1.00      0.23      0.38        13\n      banana       1.00      0.24      0.38        17\n   blackgram       1.00      0.19      0.32        16\n    chickpea       1.00      0.05      0.09        21\n     coconut       1.00      0.05      0.09        21\n      coffee       0.00      0.00      0.00        22\n      cotton       1.00      0.05      0.10        20\n      grapes       1.00      0.06      0.11        18\n        jute       1.00      0.07      0.13        28\n kidneybeans       0.03      1.00      0.07        14\n      lentil       0.00      0.00      0.00        23\n       maize       0.00      0.00      0.00        21\n       mango       0.00      0.00      0.00        26\n   mothbeans       0.00      0.00      0.00        19\n    mungbean       1.00      0.12      0.22        24\n   muskmelon       1.00      0.30      0.47        23\n      orange       1.00      0.03      0.07        29\n      papaya       1.00      0.05      0.10        19\n  pigeonpeas       0.00      0.00      0.00        18\n pomegranate       1.00      0.12      0.21        17\n        rice       0.50      0.06      0.11        16\n  watermelon       1.00      0.13      0.24        15\n\n    accuracy                           0.11       440\n   macro avg       0.66      0.13      0.14       440\nweighted avg       0.66      0.11      0.13       440\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cross validation score (SVM)\nscore = cross_val_score(SVM,features,target,cv=5)\nscore","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:55:38.329822Z","iopub.execute_input":"2024-02-01T05:55:38.330369Z","iopub.status.idle":"2024-02-01T05:55:40.262205Z","shell.execute_reply.started":"2024-02-01T05:55:38.330332Z","shell.execute_reply":"2024-02-01T05:55:40.260556Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"array([0.27727273, 0.28863636, 0.29090909, 0.275     , 0.26818182])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Logistic Regression\n\nLogistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n\nLogistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLogReg = LogisticRegression(random_state=2)\n\nLogReg.fit(Xtrain,Ytrain)\n\npredicted_values = LogReg.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('Logistic Regression')\nprint(\"Logistic Regression's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:58:52.712393Z","iopub.execute_input":"2024-02-01T05:58:52.712814Z","iopub.status.idle":"2024-02-01T05:58:53.053627Z","shell.execute_reply.started":"2024-02-01T05:58:52.712758Z","shell.execute_reply":"2024-02-01T05:58:53.052777Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Logistic Regression's Accuracy is:  0.9522727272727273\n              precision    recall  f1-score   support\n\n       apple       1.00      1.00      1.00        13\n      banana       1.00      1.00      1.00        17\n   blackgram       0.86      0.75      0.80        16\n    chickpea       1.00      1.00      1.00        21\n     coconut       1.00      1.00      1.00        21\n      coffee       1.00      1.00      1.00        22\n      cotton       0.86      0.90      0.88        20\n      grapes       1.00      1.00      1.00        18\n        jute       0.84      0.93      0.88        28\n kidneybeans       1.00      1.00      1.00        14\n      lentil       0.88      1.00      0.94        23\n       maize       0.90      0.86      0.88        21\n       mango       0.96      1.00      0.98        26\n   mothbeans       0.84      0.84      0.84        19\n    mungbean       1.00      0.96      0.98        24\n   muskmelon       1.00      1.00      1.00        23\n      orange       1.00      1.00      1.00        29\n      papaya       1.00      0.95      0.97        19\n  pigeonpeas       1.00      1.00      1.00        18\n pomegranate       1.00      1.00      1.00        17\n        rice       0.85      0.69      0.76        16\n  watermelon       1.00      1.00      1.00        15\n\n    accuracy                           0.95       440\n   macro avg       0.95      0.95      0.95       440\nweighted avg       0.95      0.95      0.95       440\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cross validation score (Logistic Regression)\nscore = cross_val_score(LogReg,features,target,cv=5)\nscore","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:58:58.215688Z","iopub.execute_input":"2024-02-01T05:58:58.216131Z","iopub.status.idle":"2024-02-01T05:58:59.994479Z","shell.execute_reply.started":"2024-02-01T05:58:58.216089Z","shell.execute_reply":"2024-02-01T05:58:59.993584Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"array([0.95      , 0.96590909, 0.94772727, 0.96818182, 0.94318182])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving trained Logistic Regression model","metadata":{}},{"cell_type":"code","source":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nLR_pkl_filename = 'LogisticRegression.pkl'\n# Open the file to save as pkl file\nLR_Model_pkl = open(DT_pkl_filename, 'wb')\npickle.dump(LogReg, LR_Model_pkl)\n# Close the pickle instances\nLR_Model_pkl.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T05:59:07.457155Z","iopub.execute_input":"2024-02-01T05:59:07.457903Z","iopub.status.idle":"2024-02-01T05:59:07.465016Z","shell.execute_reply.started":"2024-02-01T05:59:07.457848Z","shell.execute_reply":"2024-02-01T05:59:07.464040Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest\n\n1)n_estimators: \n-Represents the number of trees in the forest.\n-More trees can potentially improve the model's performance but increase computational cost.\n\n2)max_depth:\n-Specifies the maximum depth of each decision tree in the forest.\n-Deeper trees can capture more complex patterns but may lead to overfitting.\n\n3)max_features:\n-Determines the maximum number of features considered for splitting a node.\n-Controlling feature selection helps in decorrelating trees and improving generalization.\n\n4)min_samples_split:\n-Sets the minimum number of samples required to split an internal node.\n-Larger values prevent splitting nodes with a small number of samples, reducing overfitting.\n\n5)min_samples_leaf:\n-Specifies the minimum number of samples required to be at a leaf node.\n-Larger values result in smaller leaves, preventing the model from becoming too specific to the training data.\n\n6)max_samples:\n-Represents the maximum number or fraction of samples used for training each tree.\n-It introduces randomness by training each tree on a subset of the data, improving model diversity.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=20, random_state=0)\nRF.fit(Xtrain,Ytrain)\n\npredicted_values = RF.predict(Xtest)\n\nx = metrics.accuracy_score(Ytest, predicted_values)\nacc.append(x)\nmodel.append('RF')\nprint(\"RF's Accuracy is: \", x)\n\nprint(classification_report(Ytest,predicted_values))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:08:00.139982Z","iopub.execute_input":"2024-02-01T06:08:00.140395Z","iopub.status.idle":"2024-02-01T06:08:00.311988Z","shell.execute_reply.started":"2024-02-01T06:08:00.140362Z","shell.execute_reply":"2024-02-01T06:08:00.311107Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"RF's Accuracy is:  0.990909090909091\n              precision    recall  f1-score   support\n\n       apple       1.00      1.00      1.00        13\n      banana       1.00      1.00      1.00        17\n   blackgram       0.94      1.00      0.97        16\n    chickpea       1.00      1.00      1.00        21\n     coconut       1.00      1.00      1.00        21\n      coffee       1.00      1.00      1.00        22\n      cotton       1.00      1.00      1.00        20\n      grapes       1.00      1.00      1.00        18\n        jute       0.90      1.00      0.95        28\n kidneybeans       1.00      1.00      1.00        14\n      lentil       1.00      1.00      1.00        23\n       maize       1.00      1.00      1.00        21\n       mango       1.00      1.00      1.00        26\n   mothbeans       1.00      0.95      0.97        19\n    mungbean       1.00      1.00      1.00        24\n   muskmelon       1.00      1.00      1.00        23\n      orange       1.00      1.00      1.00        29\n      papaya       1.00      1.00      1.00        19\n  pigeonpeas       1.00      1.00      1.00        18\n pomegranate       1.00      1.00      1.00        17\n        rice       1.00      0.81      0.90        16\n  watermelon       1.00      1.00      1.00        15\n\n    accuracy                           0.99       440\n   macro avg       0.99      0.99      0.99       440\nweighted avg       0.99      0.99      0.99       440\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cross validation score (Random Forest)\nscore = cross_val_score(RF,features,target,cv=5)\nscore","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:08:02.629107Z","iopub.execute_input":"2024-02-01T06:08:02.629498Z","iopub.status.idle":"2024-02-01T06:08:03.157195Z","shell.execute_reply.started":"2024-02-01T06:08:02.629463Z","shell.execute_reply":"2024-02-01T06:08:03.156225Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"array([0.99772727, 0.99545455, 0.99772727, 0.99318182, 0.98863636])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Saving trained Random Forest model","metadata":{}},{"cell_type":"code","source":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nRF_pkl_filename = 'RandomForest.pkl'\n# Open the file to save as pkl file\nRF_Model_pkl = open(RF_pkl_filename, 'wb')\npickle.dump(RF, RF_Model_pkl)\n# Close the pickle instances\nRF_Model_pkl.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:08:03.589437Z","iopub.execute_input":"2024-02-01T06:08:03.589841Z","iopub.status.idle":"2024-02-01T06:08:03.600109Z","shell.execute_reply.started":"2024-02-01T06:08:03.589805Z","shell.execute_reply":"2024-02-01T06:08:03.598703Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost\n\nIn this algorithm, decision trees are created in sequential form. Weights play an important role in XGBoost. Weights are assigned to all the independent variables which are then fed into the decision tree which predicts results. The weight of variables predicted wrong by the tree is increased and these variables are then fed to the second decision tree. These individual classifiers/predictors then ensemble to give a strong and more precise model. It can work on regression, classification, ranking, and user-defined prediction problems.","metadata":{}},{"cell_type":"code","source":"def XGBoost(Xtrain, Ytrain):\n    XB = xgb.XGBClassifier()\n    score = cross_val_score(XB,features,target,cv=5)\n    print(\"Cross-Validation Accuracy Scores: \", score)\n    XB.fit(Xtrain,Ytrain)\n    return XB\n\nXGBoost_model = XGBoost(Xtrain, Ytrain)\nXGBoost_model","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:23.480982Z","iopub.execute_input":"2024-02-01T06:53:23.481401Z","iopub.status.idle":"2024-02-01T06:53:26.755606Z","shell.execute_reply.started":"2024-02-01T06:53:23.481364Z","shell.execute_reply":"2024-02-01T06:53:26.754416Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"Cross-Validation Accuracy Scores:  [0.99318182 0.99318182 0.99318182 0.99090909 0.99090909]\n","output_type":"stream"},{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)"},"metadata":{}}]},{"cell_type":"code","source":"def predict_XGBoost(XB, Xtest, Ytest):\n    predicted_values = XB.predict(Xtest)\n    x = metrics.accuracy_score(Ytest, predicted_values)\n    print(\"XGBoost's Accuracy is: \", x)\n    \npred_XGBoost = predict_XGBoost(XB, Xtest, Ytest)\npred_XGBoost","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:26.757920Z","iopub.execute_input":"2024-02-01T06:53:26.758724Z","iopub.status.idle":"2024-02-01T06:53:26.786069Z","shell.execute_reply.started":"2024-02-01T06:53:26.758669Z","shell.execute_reply":"2024-02-01T06:53:26.784864Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"XGBoost's Accuracy is:  0.031818181818181815\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Saving trained XGBoost model","metadata":{}},{"cell_type":"code","source":"import pickle\n# Dump the trained Naive Bayes classifier with Pickle\nXB_pkl_filename = 'XGBoost.pkl'\n# Open the file to save as pkl file\nXB_Model_pkl = open(XB_pkl_filename, 'wb')\npickle.dump(XB, XB_Model_pkl)\n# Close the pickle instances\nXB_Model_pkl.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:30.234963Z","iopub.execute_input":"2024-02-01T06:53:30.235390Z","iopub.status.idle":"2024-02-01T06:53:30.245513Z","shell.execute_reply.started":"2024-02-01T06:53:30.235352Z","shell.execute_reply":"2024-02-01T06:53:30.243928Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy Comparison","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=[10,5],dpi = 100)\n# plt.title('Accuracy Comparison')\n# plt.xlabel('Accuracy')\n# plt.ylabel('Algorithm')\n# sns.barplot(x = acc,y = model,palette='dark')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:52.616716Z","iopub.execute_input":"2024-02-01T06:53:52.617179Z","iopub.status.idle":"2024-02-01T06:53:52.622264Z","shell.execute_reply.started":"2024-02-01T06:53:52.617139Z","shell.execute_reply":"2024-02-01T06:53:52.620961Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"# accuracy_models = dict(zip(model, acc))\n# for k, v in accuracy_models.items():\n#     print (k, '-->', v)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:53:59.136893Z","iopub.execute_input":"2024-02-01T06:53:59.137284Z","iopub.status.idle":"2024-02-01T06:53:59.142025Z","shell.execute_reply.started":"2024-02-01T06:53:59.137250Z","shell.execute_reply":"2024-02-01T06:53:59.140753Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"## Making a prediction","metadata":{}},{"cell_type":"code","source":"Xtest","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:54:04.032783Z","iopub.execute_input":"2024-02-01T06:54:04.033208Z","iopub.status.idle":"2024-02-01T06:54:04.054392Z","shell.execute_reply.started":"2024-02-01T06:54:04.033173Z","shell.execute_reply":"2024-02-01T06:54:04.053245Z"},"trusted":true},"execution_count":142,"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"             N         P         K  temperature  humidity        ph  rainfall\n2121  0.879141 -0.981331 -0.397917    -0.009632 -0.493708  1.289054  1.579047\n960  -1.342542 -0.799394 -0.239928    -0.322020  0.982112 -1.013858  0.027802\n952  -0.746481 -1.466498 -0.081939    -0.870895  1.023475  0.896302  0.074663\n1958  1.773234 -0.041322 -0.575655    -0.528084  0.174759 -0.458738 -0.662168\n681  -1.207074 -0.496165 -0.615152     0.487957  0.398418  0.376362 -1.188786\n...        ...       ...       ...          ...       ...       ...       ...\n1684 -1.179980 -1.102623 -0.753392    -3.052174  0.886906 -0.004610  0.052759\n1477  0.960423 -1.072300 -0.062190     0.661641  0.864224  0.125722 -1.477913\n851  -1.207074  0.322553 -0.496660    -0.450479 -0.183172  0.769984 -1.224420\n370  -0.367169  0.079970 -0.457163    -0.703260 -2.330678 -0.897854  0.423378\n2010  1.041704 -0.010999 -0.081939    -0.144060  0.019558  1.098859  0.851496\n\n[440 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2121</th>\n      <td>0.879141</td>\n      <td>-0.981331</td>\n      <td>-0.397917</td>\n      <td>-0.009632</td>\n      <td>-0.493708</td>\n      <td>1.289054</td>\n      <td>1.579047</td>\n    </tr>\n    <tr>\n      <th>960</th>\n      <td>-1.342542</td>\n      <td>-0.799394</td>\n      <td>-0.239928</td>\n      <td>-0.322020</td>\n      <td>0.982112</td>\n      <td>-1.013858</td>\n      <td>0.027802</td>\n    </tr>\n    <tr>\n      <th>952</th>\n      <td>-0.746481</td>\n      <td>-1.466498</td>\n      <td>-0.081939</td>\n      <td>-0.870895</td>\n      <td>1.023475</td>\n      <td>0.896302</td>\n      <td>0.074663</td>\n    </tr>\n    <tr>\n      <th>1958</th>\n      <td>1.773234</td>\n      <td>-0.041322</td>\n      <td>-0.575655</td>\n      <td>-0.528084</td>\n      <td>0.174759</td>\n      <td>-0.458738</td>\n      <td>-0.662168</td>\n    </tr>\n    <tr>\n      <th>681</th>\n      <td>-1.207074</td>\n      <td>-0.496165</td>\n      <td>-0.615152</td>\n      <td>0.487957</td>\n      <td>0.398418</td>\n      <td>0.376362</td>\n      <td>-1.188786</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>-1.179980</td>\n      <td>-1.102623</td>\n      <td>-0.753392</td>\n      <td>-3.052174</td>\n      <td>0.886906</td>\n      <td>-0.004610</td>\n      <td>0.052759</td>\n    </tr>\n    <tr>\n      <th>1477</th>\n      <td>0.960423</td>\n      <td>-1.072300</td>\n      <td>-0.062190</td>\n      <td>0.661641</td>\n      <td>0.864224</td>\n      <td>0.125722</td>\n      <td>-1.477913</td>\n    </tr>\n    <tr>\n      <th>851</th>\n      <td>-1.207074</td>\n      <td>0.322553</td>\n      <td>-0.496660</td>\n      <td>-0.450479</td>\n      <td>-0.183172</td>\n      <td>0.769984</td>\n      <td>-1.224420</td>\n    </tr>\n    <tr>\n      <th>370</th>\n      <td>-0.367169</td>\n      <td>0.079970</td>\n      <td>-0.457163</td>\n      <td>-0.703260</td>\n      <td>-2.330678</td>\n      <td>-0.897854</td>\n      <td>0.423378</td>\n    </tr>\n    <tr>\n      <th>2010</th>\n      <td>1.041704</td>\n      <td>-0.010999</td>\n      <td>-0.081939</td>\n      <td>-0.144060</td>\n      <td>0.019558</td>\n      <td>1.098859</td>\n      <td>0.851496</td>\n    </tr>\n  </tbody>\n</table>\n<p>440 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]])\ndata = pd.DataFrame([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]], columns = Xtrain.columns)\nprediction = XB.predict(data)\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:54:05.544897Z","iopub.execute_input":"2024-02-01T06:54:05.545321Z","iopub.status.idle":"2024-02-01T06:54:05.561428Z","shell.execute_reply.started":"2024-02-01T06:54:05.545286Z","shell.execute_reply":"2024-02-01T06:54:05.560045Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"['coffee']\n","output_type":"stream"}]},{"cell_type":"code","source":"# data = np.array([[83, 45, 60, 28, 70.3, 7.0, 150.9]])\ndata = pd.DataFrame([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]], columns = Xtrain.columns)\nprediction = RF.predict(data)\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T06:54:07.282762Z","iopub.execute_input":"2024-02-01T06:54:07.283196Z","iopub.status.idle":"2024-02-01T06:54:07.298110Z","shell.execute_reply.started":"2024-02-01T06:54:07.283154Z","shell.execute_reply":"2024-02-01T06:54:07.296693Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"['coffee']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}